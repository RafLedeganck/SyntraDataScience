{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HR Churn - Eindwerk Data Science 2023-24 - Raf Ledeganck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>(Development on Tensorflow 2.11.0 Docker Container)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.23.4)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.5.0\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.5 MB 687 kB/s eta 0:00:01    |████████                        | 8.6 MB 3.9 MB/s eta 0:00:07     |█████████████▊                  | 14.8 MB 718 kB/s eta 0:00:28\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: joblib, scipy, threadpoolctl, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.3.2 scipy-1.10.1 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[K     |████████████████████████████████| 505 kB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from pandas) (1.23.4)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[K     |████████████████████████████████| 345 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 pytz-2024.1 tzdata-2024.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.8/dist-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.23.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.8/dist-packages (from seaborn) (3.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.38.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting scikit-optimize\n",
      "  Downloading scikit_optimize-0.10.1-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-optimize) (1.3.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-optimize) (1.4.2)\n",
      "Collecting pyaml>=16.9\n",
      "  Downloading pyaml-24.4.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-optimize) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from scikit-optimize) (1.23.4)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.8/dist-packages (from scikit-optimize) (21.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
      "Collecting PyYAML\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736 kB)\n",
      "\u001b[K     |████████████████████████████████| 736 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=21.3->scikit-optimize) (3.0.9)\n",
      "Installing collected packages: PyYAML, pyaml, scikit-optimize\n",
      "Successfully installed PyYAML-6.0.1 pyaml-24.4.0 scikit-optimize-0.10.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.2-py3-none-any.whl (257 kB)\n",
      "\u001b[K     |████████████████████████████████| 257 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn) (1.23.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn) (3.5.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn) (1.4.2)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.2\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting scikeras\n",
      "  Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.8/dist-packages (from scikeras) (21.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikeras) (1.3.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=0.21->scikeras) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.5.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.10.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: scikeras\n",
      "Successfully installed scikeras-0.12.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting feature_engine\n",
      "  Downloading feature_engine-1.6.2-py2.py3-none-any.whl (328 kB)\n",
      "\u001b[K     |████████████████████████████████| 328 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.23.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.3.2)\n",
      "Collecting statsmodels>=0.11.1\n",
      "  Downloading statsmodels-0.14.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.9 MB 3.5 MB/s eta 0:00:01    |██████████▍                     | 3.5 MB 697 kB/s eta 0:00:11     |████████████▋                   | 4.3 MB 697 kB/s eta 0:00:10\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (2.0.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->feature_engine) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->feature_engine) (3.5.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.8/dist-packages (from statsmodels>=0.11.1->feature_engine) (21.3)\n",
      "Collecting patsy>=0.5.4\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[K     |████████████████████████████████| 233 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.3->feature_engine) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.3->feature_engine) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.3->feature_engine) (2024.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=21.3->statsmodels>=0.11.1->feature_engine) (3.0.9)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5.4->statsmodels>=0.11.1->feature_engine) (1.16.0)\n",
      "Installing collected packages: patsy, statsmodels, feature-engine\n",
      "Successfully installed feature-engine-1.6.2 patsy-0.5.6 statsmodels-0.14.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "#!pip install scikit-image\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install scikit-optimize\n",
    "!pip install imbalanced-learn\n",
    "!pip install scikeras\n",
    "!pip install feature_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "import sklearn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
    "#from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC # Support Vector Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scipy\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow, Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adagrad\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, BinaryFocalCrossentropy\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropConstantFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import sklearn\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "#from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display_html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_NaN_via_dupl(df, nan_cols, dup_colset):\n",
    "    \"\"\"\n",
    "    1) Checks whether a row with a NaN value has duplicates by comparing it with similar rows.\n",
    "    2) If all duplicate rows share the same value for the column with NaN, then the NaN is replaced with this value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Dataframe in which NaN values are to be replaced.\n",
    "    nan_cols: list\n",
    "        List of columns in which we want to replace the NaN\n",
    "        Example: ['EMPLOYEE_CLASS', 'EMPLOYEE_LEVEL']\n",
    "    dup_colset: list\n",
    "        List of columns used to derive a fill value for the NaN\n",
    "        If a row has a NaN value in a specified column, the function checks for similar rows by comparing the values in this\n",
    "        column set.  If all columns in this set have an identical value to the row with NaN, then the rows are considered\n",
    "        'similar'.\n",
    "        Example: ['EMPLOYEE_TYPE', 'DEPARTMENT', 'JOB', 'COMPANY', 'SITE', 'PERMANENT', 'EMPLOYEE_CLASS', 'EMPLOYEE_LEVEL']\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df_stats: DataFrame\n",
    "        Statistics on the performance of the function\n",
    "        - 'col': column name\n",
    "        - 'nan': no. of lines with NaN\n",
    "        - 'fill': no. of NaN replaced with value from similar lines\n",
    "        - 'mult': no. of NaN lines not filled because similar lines have multiple values and look-up is inconclusive\n",
    "        - 'nodup': no. of NaN lines not filled because there are no similar lines to derive a replacement value from\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    col_lst, nan_lst, fill_lst, mult_lst, nodup_lst = list(), list(), list(), list(), list()\n",
    "    \n",
    "    # Function assumes NAN_COLS is a subset of DUP_COLSET.  If this is not the case, it is enforced here.\n",
    "    dup_colset = dup_colset + nan_cols\n",
    "    dup_colset = np.unique(dup_colset).tolist()\n",
    "\n",
    "    # Look-up takes 10 columns as input.  If less than 10 columns were given, add empty ones.\n",
    "    df_subset = df[dup_colset].copy()\n",
    "    i = len(df_subset.columns)\n",
    "    if i > 11:\n",
    "        raise Exception(\"Max. 10 columns can be used to find similar rows.\")\n",
    "    while i < 11:\n",
    "        i += 1\n",
    "        col_name = 'EMPTY' + str(i)\n",
    "        df_subset[col_name] = \"0\"\n",
    "        dup_colset.append(col_name)\n",
    "        \n",
    "    for col in nan_cols:\n",
    "        count_nan = 0\n",
    "        count_fill = 0\n",
    "        count_mult = 0\n",
    "        count_nodup = 0\n",
    "        dup_cols = dup_colset.copy()\n",
    "        dup_cols.remove(col)\n",
    "\n",
    "        # Build a Series which indicates for each line in DF whether it has a duplicate row \n",
    "        # considering all columns except COL\n",
    "        ser_dup = df_subset.duplicated(subset=dup_cols, keep=False)\n",
    "\n",
    "        # Build a list with the index of all rows in DF that have NaN for COL\n",
    "        lst_na = df_subset.loc[pd.isna(df[col]), :].index\n",
    "\n",
    "        for i in lst_na:\n",
    "            count_nan += 1\n",
    "\n",
    "            # For each row where COL == NaN, check if it has a duplicate\n",
    "            if ser_dup.iloc[i] == True:\n",
    "\n",
    "                # Row has duplicate.\n",
    "                # SEARCH_LINE is the row for which we aim to remove the NaN\n",
    "                search_line = df_subset.iloc[i, :]\n",
    "\n",
    "                # Build dataframe with all duplicate lines for SEARCH_LINE ignoring COL\n",
    "                df_dup = df_subset.loc[(df_subset[dup_cols[0]] == search_line[dup_cols[0]]) &   \n",
    "                                       (df_subset[dup_cols[1]] == search_line[dup_cols[1]]) &\n",
    "                                       (df_subset[dup_cols[2]] == search_line[dup_cols[2]]) &\n",
    "                                       (df_subset[dup_cols[3]] == search_line[dup_cols[3]]) &\n",
    "                                       (df_subset[dup_cols[4]] == search_line[dup_cols[4]]) &\n",
    "                                       (df_subset[dup_cols[5]] == search_line[dup_cols[5]]) &\n",
    "                                       (df_subset[dup_cols[6]] == search_line[dup_cols[6]]) &\n",
    "                                       (df_subset[dup_cols[7]] == search_line[dup_cols[7]]) &\n",
    "                                       (df_subset[dup_cols[8]] == search_line[dup_cols[8]]) &\n",
    "                                       (df_subset[dup_cols[9]] == search_line[dup_cols[9]])\n",
    "                                        ]\n",
    "\n",
    "                # Check if all lines in DF_DUP have same value in COL, if so we assume we can replace NaN with this value\n",
    "                if len(df_dup[col].value_counts()) == 1:\n",
    "                    # All duplicate lines have the same value for the NaN field so we replace NaN with this value\n",
    "                    df.iloc[i, df.columns.tolist().index(col)] = \\\n",
    "                                      df_dup[df_dup[col].notna()].iloc[0, df_subset.columns.tolist().index(col)]\n",
    "                    count_fill += 1\n",
    "                else:\n",
    "                    # Multiple values for the NaN field, result is inconclusive, NaN is left\n",
    "                    count_mult += 1     \n",
    "\n",
    "            else:\n",
    "                # No duplicate row for SEARCH_LINE, NaN is left\n",
    "                count_nodup += 1\n",
    "\n",
    "        col_lst.append(col)\n",
    "        nan_lst.append(count_nan)\n",
    "        fill_lst.append(count_fill)\n",
    "        mult_lst.append(count_mult)\n",
    "        nodup_lst.append(count_nodup)\n",
    "    \n",
    "    stats = {'col' : col_lst,\n",
    "             'nan' : nan_lst,\n",
    "             'fill' : fill_lst,\n",
    "             'mult' : mult_lst,\n",
    "             'nodup' : nodup_lst\n",
    "            }\n",
    "    return pd.DataFrame.from_dict(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_feat_imp(df_feat_imp):\n",
    "\n",
    "    df_abs = df_feat_imp.abs().copy()\n",
    "\n",
    "    # Calculate total correlation per feature rather than by OneHotEncoder split\n",
    "    lst_row= ['MANAGER', 'DEPARTMENT', 'DPT_CHANGE_FLAG', 'COMPANY', 'SITE', 'FULL_TIME', 'EMPLOYEE_LEVEL', 'HANDICAP', 'CITIZENSHIP']\n",
    "    for row in lst_row:\n",
    "        prefix = 'cat__' + row\n",
    "\n",
    "        # Add row with total correlation\n",
    "        df_abs.loc[row] = df_abs.filter(regex=prefix, axis='index').sum().values[0]\n",
    "\n",
    "        # Remove rows with correlations for individual OneHotEncoder splits\n",
    "        df_abs.drop(list(df_abs[df_abs.index.str.startswith(prefix)].index), axis='index', inplace=True)\n",
    "\n",
    "    # Remove prefixes from row names\n",
    "    dict_rownames = {}\n",
    "    for row in df_abs.index:\n",
    "        if row.find('__') > 0:\n",
    "            dict_rownames[row] = row[ (row.find('__')+2) : ]\n",
    "\n",
    "    df_abs.rename(index=dict_rownames, inplace=True)\n",
    "\n",
    "    return(df_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feat_imp(df_feat_imp, export=False, file_name='plot.jpg', show=True):\n",
    "\n",
    "    # Sort from high to low importance\n",
    "    df_feat_imp.sort_values('Importance', ascending=False, inplace=True)\n",
    "\n",
    "    # Generate plot\n",
    "    fig, ax = plt.subplots(figsize=(12,6),dpi=100)\n",
    "    sns.barplot(df_feat_imp, x=df_feat_imp.index, y=df_feat_imp['Importance'])\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Export plot\n",
    "    if export:\n",
    "        plt.savefig(file_name, bbox_inches='tight')\n",
    "\n",
    "    # Print plot\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    plt.close(fig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Employee ID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>FIRST_NAME</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>MANAGER</th>\n",
       "      <th>EMPLOYEE_TYPE</th>\n",
       "      <th>DEPARTMENT</th>\n",
       "      <th>DPT_CHANGE_FLAG</th>\n",
       "      <th>JOB</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>...</th>\n",
       "      <th>EVOLUTION_BONUS_LANGUAGE</th>\n",
       "      <th>EVOLUTION_BONUS_MISC</th>\n",
       "      <th>EVOLUTION_BONUS_OBJECTIVE</th>\n",
       "      <th>EVOLUTION_BONUS_SHARING</th>\n",
       "      <th>EVOLUTION_BONUS_TECHNICAL</th>\n",
       "      <th>EVOLUTION_BONUS_TOTAL</th>\n",
       "      <th>EVOLUTION_BONUS_UNEXPECTED</th>\n",
       "      <th>EVOLUTION_BONUS_WELCOME</th>\n",
       "      <th>EVOLUTION_BONUS_YIELD</th>\n",
       "      <th>Target_Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33675</td>\n",
       "      <td>Reverdy</td>\n",
       "      <td>ELEANA</td>\n",
       "      <td>F</td>\n",
       "      <td>212.0</td>\n",
       "      <td>EMP</td>\n",
       "      <td>1135</td>\n",
       "      <td>R</td>\n",
       "      <td>CLIENT AVISOR</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.668724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.371666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.300012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35535</td>\n",
       "      <td>Ravet</td>\n",
       "      <td>CHRISTIAN</td>\n",
       "      <td>M</td>\n",
       "      <td>11780.0</td>\n",
       "      <td>EMP</td>\n",
       "      <td>1332</td>\n",
       "      <td>R</td>\n",
       "      <td>CLIENT AVISOR</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.915450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.644679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.624565</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35826</td>\n",
       "      <td>Mandon</td>\n",
       "      <td>ADEL</td>\n",
       "      <td>M</td>\n",
       "      <td>36149.0</td>\n",
       "      <td>EMP</td>\n",
       "      <td>2055</td>\n",
       "      <td>R</td>\n",
       "      <td>CLIENT AVISOR</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.174547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.016536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35352</td>\n",
       "      <td>Ch</td>\n",
       "      <td>MOHAMMED</td>\n",
       "      <td>F</td>\n",
       "      <td>2309.0</td>\n",
       "      <td>EMP</td>\n",
       "      <td>1802</td>\n",
       "      <td>R</td>\n",
       "      <td>CLIENT AVISOR</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.852381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.852381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35997</td>\n",
       "      <td>Grand</td>\n",
       "      <td>JASSIM</td>\n",
       "      <td>M</td>\n",
       "      <td>22035.0</td>\n",
       "      <td>EMP</td>\n",
       "      <td>2055</td>\n",
       "      <td>R</td>\n",
       "      <td>CLIENT AVISOR</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Employee ID     NAME FIRST_NAME GENDER  MANAGER EMPLOYEE_TYPE  DEPARTMENT  \\\n",
       "0        33675  Reverdy     ELEANA      F    212.0           EMP        1135   \n",
       "1        35535    Ravet  CHRISTIAN      M  11780.0           EMP        1332   \n",
       "2        35826   Mandon       ADEL      M  36149.0           EMP        2055   \n",
       "3        35352       Ch   MOHAMMED      F   2309.0           EMP        1802   \n",
       "4        35997    Grand     JASSIM      M  22035.0           EMP        2055   \n",
       "\n",
       "  DPT_CHANGE_FLAG            JOB STATUS  ...  EVOLUTION_BONUS_LANGUAGE  \\\n",
       "0               R  CLIENT AVISOR      A  ...                       NaN   \n",
       "1               R  CLIENT AVISOR      A  ...                       NaN   \n",
       "2               R  CLIENT AVISOR      A  ...                       NaN   \n",
       "3               R  CLIENT AVISOR      A  ...                       NaN   \n",
       "4               R  CLIENT AVISOR      A  ...                       NaN   \n",
       "\n",
       "  EVOLUTION_BONUS_MISC EVOLUTION_BONUS_OBJECTIVE EVOLUTION_BONUS_SHARING  \\\n",
       "0                  NaN                  0.668724                     NaN   \n",
       "1                  NaN                  0.915450                     NaN   \n",
       "2                  NaN                  1.174547                     NaN   \n",
       "3                  NaN                  1.852381                     NaN   \n",
       "4                  NaN                  0.000000                     NaN   \n",
       "\n",
       "  EVOLUTION_BONUS_TECHNICAL EVOLUTION_BONUS_TOTAL EVOLUTION_BONUS_UNEXPECTED  \\\n",
       "0                       NaN              0.371666                        NaN   \n",
       "1                       NaN              0.644679                        NaN   \n",
       "2                       NaN              1.016536                        NaN   \n",
       "3                       NaN              1.852381                        NaN   \n",
       "4                       NaN              0.001837                        NaN   \n",
       "\n",
       "  EVOLUTION_BONUS_WELCOME  EVOLUTION_BONUS_YIELD  Target_Churn  \n",
       "0                     NaN               0.300012             0  \n",
       "1                     NaN               0.624565             0  \n",
       "2                     NaN                    NaN             0  \n",
       "3                     NaN                    NaN             1  \n",
       "4                     NaN                    NaN             0  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_raw = pd.read_csv(\"Data/Employee_Churn_train.csv\", sep=';')\n",
    "#df_train_raw = pd.read_csv('Employee_Churn_train.csv', sep=';')\n",
    "df_train_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Employee ID</th>\n",
       "      <th>MANAGER</th>\n",
       "      <th>DEPARTMENT</th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>AGE</th>\n",
       "      <th>CONTRACT_TENURE</th>\n",
       "      <th>EMPLOYEE_TENURE</th>\n",
       "      <th>SUM_BONUS_UNEXPECTED_3Mago</th>\n",
       "      <th>SUM_BONUS_WELCOME_3Mago</th>\n",
       "      <th>SUM_BONUS_CHALLENGE_3Mago</th>\n",
       "      <th>...</th>\n",
       "      <th>EVOLUTION_BONUS_LANGUAGE</th>\n",
       "      <th>EVOLUTION_BONUS_MISC</th>\n",
       "      <th>EVOLUTION_BONUS_OBJECTIVE</th>\n",
       "      <th>EVOLUTION_BONUS_SHARING</th>\n",
       "      <th>EVOLUTION_BONUS_TECHNICAL</th>\n",
       "      <th>EVOLUTION_BONUS_TOTAL</th>\n",
       "      <th>EVOLUTION_BONUS_UNEXPECTED</th>\n",
       "      <th>EVOLUTION_BONUS_WELCOME</th>\n",
       "      <th>EVOLUTION_BONUS_YIELD</th>\n",
       "      <th>Target_Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4741.000000</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>4741.000000</td>\n",
       "      <td>4741.000000</td>\n",
       "      <td>4741.000000</td>\n",
       "      <td>4741.000000</td>\n",
       "      <td>4741.000000</td>\n",
       "      <td>3901.000000</td>\n",
       "      <td>3901.00000</td>\n",
       "      <td>3901.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1891.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3411.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1935.000000</td>\n",
       "      <td>4741.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>34434.665893</td>\n",
       "      <td>13691.126115</td>\n",
       "      <td>1575.311960</td>\n",
       "      <td>103.156929</td>\n",
       "      <td>30.519933</td>\n",
       "      <td>1.045349</td>\n",
       "      <td>3.669479</td>\n",
       "      <td>14.743655</td>\n",
       "      <td>2.81979</td>\n",
       "      <td>216.859282</td>\n",
       "      <td>...</td>\n",
       "      <td>1.036458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.561783</td>\n",
       "      <td>0.128553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.717623</td>\n",
       "      <td>1.505207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.442746</td>\n",
       "      <td>0.136891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16220.648895</td>\n",
       "      <td>12053.696041</td>\n",
       "      <td>300.026668</td>\n",
       "      <td>2.346073</td>\n",
       "      <td>7.363317</td>\n",
       "      <td>0.234768</td>\n",
       "      <td>2.777664</td>\n",
       "      <td>207.441647</td>\n",
       "      <td>53.03357</td>\n",
       "      <td>745.333554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.126128</td>\n",
       "      <td>0.278075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.613516</td>\n",
       "      <td>1.346645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.844412</td>\n",
       "      <td>0.343768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1025.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22309.000000</td>\n",
       "      <td>2635.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.588570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.770377</td>\n",
       "      <td>0.997499</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.729704</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38751.000000</td>\n",
       "      <td>10284.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.055409</td>\n",
       "      <td>1.018913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.022007</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47514.000000</td>\n",
       "      <td>23807.000000</td>\n",
       "      <td>1884.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.744527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.649027</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.561656</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>57145.000000</td>\n",
       "      <td>56574.000000</td>\n",
       "      <td>2055.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>4200.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>8937.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1074.187817</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1074.187817</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.660723</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Employee ID       MANAGER   DEPARTMENT      COMPANY          AGE  \\\n",
       "count   4741.000000   4710.000000  4741.000000  4741.000000  4741.000000   \n",
       "mean   34434.665893  13691.126115  1575.311960   103.156929    30.519933   \n",
       "std    16220.648895  12053.696041   300.026668     2.346073     7.363317   \n",
       "min        3.000000     80.000000  1025.000000   100.000000    18.000000   \n",
       "25%    22309.000000   2635.000000  1266.000000   102.000000    25.000000   \n",
       "50%    38751.000000  10284.000000  1567.000000   102.000000    29.000000   \n",
       "75%    47514.000000  23807.000000  1884.000000   105.000000    34.000000   \n",
       "max    57145.000000  56574.000000  2055.000000   108.000000    61.000000   \n",
       "\n",
       "       CONTRACT_TENURE  EMPLOYEE_TENURE  SUM_BONUS_UNEXPECTED_3Mago  \\\n",
       "count      4741.000000      4741.000000                 3901.000000   \n",
       "mean          1.045349         3.669479                   14.743655   \n",
       "std           0.234768         2.777664                  207.441647   \n",
       "min           1.000000         1.000000                    0.000000   \n",
       "25%           1.000000         1.000000                    0.000000   \n",
       "50%           1.000000         3.000000                    0.000000   \n",
       "75%           1.000000         5.000000                    0.000000   \n",
       "max           4.000000        13.000000                 4200.000000   \n",
       "\n",
       "       SUM_BONUS_WELCOME_3Mago  SUM_BONUS_CHALLENGE_3Mago  ...  \\\n",
       "count               3901.00000                3901.000000  ...   \n",
       "mean                   2.81979                 216.859282  ...   \n",
       "std                   53.03357                 745.333554  ...   \n",
       "min                    0.00000                   0.000000  ...   \n",
       "25%                    0.00000                   0.000000  ...   \n",
       "50%                    0.00000                   0.000000  ...   \n",
       "75%                    0.00000                   0.000000  ...   \n",
       "max                 1000.00000                8937.000000  ...   \n",
       "\n",
       "       EVOLUTION_BONUS_LANGUAGE  EVOLUTION_BONUS_MISC  \\\n",
       "count                160.000000                   0.0   \n",
       "mean                   1.036458                   NaN   \n",
       "std                    0.443003                   NaN   \n",
       "min                    0.000000                   NaN   \n",
       "25%                    1.000000                   NaN   \n",
       "50%                    1.000000                   NaN   \n",
       "75%                    1.000000                   NaN   \n",
       "max                    3.000000                   NaN   \n",
       "\n",
       "       EVOLUTION_BONUS_OBJECTIVE  EVOLUTION_BONUS_SHARING  \\\n",
       "count                1891.000000                20.000000   \n",
       "mean                    3.561783                 0.128553   \n",
       "std                    29.126128                 0.278075   \n",
       "min                     0.000000                 0.000000   \n",
       "25%                     0.588570                 0.000000   \n",
       "50%                     1.000000                 0.000000   \n",
       "75%                     1.744527                 0.000000   \n",
       "max                  1074.187817                 0.844444   \n",
       "\n",
       "       EVOLUTION_BONUS_TECHNICAL  EVOLUTION_BONUS_TOTAL  \\\n",
       "count                        1.0            3411.000000   \n",
       "mean                         0.0               2.717623   \n",
       "std                          NaN              21.613516   \n",
       "min                          0.0               0.000000   \n",
       "25%                          0.0               0.770377   \n",
       "50%                          0.0               1.055409   \n",
       "75%                          0.0               1.649027   \n",
       "max                          0.0            1074.187817   \n",
       "\n",
       "       EVOLUTION_BONUS_UNEXPECTED  EVOLUTION_BONUS_WELCOME  \\\n",
       "count                   22.000000                     11.0   \n",
       "mean                     1.505207                      0.0   \n",
       "std                      1.346645                      0.0   \n",
       "min                      0.238095                      0.0   \n",
       "25%                      0.997499                      0.0   \n",
       "50%                      1.018913                      0.0   \n",
       "75%                      1.300000                      0.0   \n",
       "max                      6.000000                      0.0   \n",
       "\n",
       "       EVOLUTION_BONUS_YIELD  Target_Churn  \n",
       "count            1935.000000   4741.000000  \n",
       "mean                1.442746      0.136891  \n",
       "std                 1.844412      0.343768  \n",
       "min                 0.000000      0.000000  \n",
       "25%                 0.729704      0.000000  \n",
       "50%                 1.022007      0.000000  \n",
       "75%                 1.561656      0.000000  \n",
       "max                36.660723      1.000000  \n",
       "\n",
       "[8 rows x 41 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4741 entries, 0 to 4740\n",
      "Data columns (total 55 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Employee ID                 4741 non-null   int64  \n",
      " 1   NAME                        4741 non-null   object \n",
      " 2   FIRST_NAME                  4741 non-null   object \n",
      " 3   GENDER                      4741 non-null   object \n",
      " 4   MANAGER                     4710 non-null   float64\n",
      " 5   EMPLOYEE_TYPE               4741 non-null   object \n",
      " 6   DEPARTMENT                  4741 non-null   int64  \n",
      " 7   DPT_CHANGE_FLAG             4741 non-null   object \n",
      " 8   JOB                         4741 non-null   object \n",
      " 9   STATUS                      4741 non-null   object \n",
      " 10  COMPANY                     4741 non-null   int64  \n",
      " 11  SITE                        4741 non-null   object \n",
      " 12  PERMANENT                   4741 non-null   object \n",
      " 13  EMPLOYEE_CLASS              4621 non-null   object \n",
      " 14  FULL_TIME                   4741 non-null   object \n",
      " 15  EMPLOYEE_LEVEL              4740 non-null   object \n",
      " 16  HANDICAP                    12 non-null     object \n",
      " 17  CITIZENSHIP                 4739 non-null   object \n",
      " 18  AGE                         4741 non-null   int64  \n",
      " 19  CONTRACT_TENURE             4741 non-null   int64  \n",
      " 20  EMPLOYEE_TENURE             4741 non-null   int64  \n",
      " 21  SUM_BONUS_UNEXPECTED_3Mago  3901 non-null   float64\n",
      " 22  SUM_BONUS_WELCOME_3Mago     3901 non-null   float64\n",
      " 23  SUM_BONUS_CHALLENGE_3Mago   3901 non-null   float64\n",
      " 24  SUM_BONUS_MISC_3Mago        3901 non-null   float64\n",
      " 25  SUM_BONUS_EXC_3Mago         3901 non-null   float64\n",
      " 26  SUM_BONUS_LANGUAGE_3Mago    3899 non-null   float64\n",
      " 27  SUM_BONUS_SHARING_3Mago     3901 non-null   float64\n",
      " 28  SUM_BONUS_OBJECTIVE_3Mago   3901 non-null   float64\n",
      " 29  SUM_BONUS_YIELD_3Mago       3901 non-null   float64\n",
      " 30  SUM_BONUS_TECHNICAL_3Mago   3901 non-null   float64\n",
      " 31  SUM_BONUS_TOTAL_3Mago       3901 non-null   float64\n",
      " 32  SUM_BONUS_UNEXPECTED        4583 non-null   float64\n",
      " 33  SUM_BONUS_WELCOME           4583 non-null   float64\n",
      " 34  SUM_BONUS_CHALLENGE         4583 non-null   float64\n",
      " 35  SUM_BONUS_MISC              4583 non-null   float64\n",
      " 36  SUM_BONUS_EXC               4583 non-null   float64\n",
      " 37  SUM_BONUS_LANGUAGE          4583 non-null   float64\n",
      " 38  SUM_BONUS_SHARING           4583 non-null   float64\n",
      " 39  SUM_BONUS_OBJECTIVE         4583 non-null   float64\n",
      " 40  SUM_BONUS_YIELD             4583 non-null   float64\n",
      " 41  SUM_BONUS_TECHNICAL         4583 non-null   float64\n",
      " 42  SUM_BONUS_TOTAL             4583 non-null   float64\n",
      " 43  EVOLUTION_BONUS_CHALLENGE   741 non-null    float64\n",
      " 44  EVOLUTION_BONUS_EXC         6 non-null      float64\n",
      " 45  EVOLUTION_BONUS_LANGUAGE    160 non-null    float64\n",
      " 46  EVOLUTION_BONUS_MISC        0 non-null      float64\n",
      " 47  EVOLUTION_BONUS_OBJECTIVE   1891 non-null   float64\n",
      " 48  EVOLUTION_BONUS_SHARING     20 non-null     float64\n",
      " 49  EVOLUTION_BONUS_TECHNICAL   1 non-null      float64\n",
      " 50  EVOLUTION_BONUS_TOTAL       3411 non-null   float64\n",
      " 51  EVOLUTION_BONUS_UNEXPECTED  22 non-null     float64\n",
      " 52  EVOLUTION_BONUS_WELCOME     11 non-null     float64\n",
      " 53  EVOLUTION_BONUS_YIELD       1935 non-null   float64\n",
      " 54  Target_Churn                4741 non-null   int64  \n",
      "dtypes: float64(34), int64(7), object(14)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct values per (non-numeric) column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in GENDER : ['F' 'M']\n",
      "Distinct values in EMPLOYEE_TYPE : ['EMP']\n",
      "Distinct values in DPT_CHANGE_FLAG : ['R' 'E' 'C']\n",
      "Distinct values in JOB : ['CLIENT AVISOR']\n",
      "Distinct values in STATUS : ['A']\n",
      "Distinct values in SITE : ['LYON7' 'BORDEAUX' 'PARIS3' 'LYON2' 'LYON6' 'NANTES3' 'STRASBOURG' 'NICE'\n",
      " 'PARIS' 'PARIS 2' 'LYON1' 'PARIS P 1' 'LYON8' 'NANTES4' 'NANTES2'\n",
      " 'NANTES1' 'BORDEAUX 2']\n",
      "Distinct values in PERMANENT : ['R' 'E']\n",
      "Distinct values in EMPLOYEE_CLASS : ['INT' nan 'IMP']\n",
      "Distinct values in FULL_TIME : ['F' 'E' 'P']\n",
      "Distinct values in EMPLOYEE_LEVEL : ['C' 'A' 'B' 'T' nan]\n",
      "Distinct values in HANDICAP : [nan 'MOTL' 'MOTD' 'VISU']\n",
      "Distinct values in CITIZENSHIP : ['FRA' 'SWE' 'BEL' 'DEU' 'PRT' 'IRL' 'SRB' 'TUN' 'LUX' 'NDL' 'MAR' 'ESP'\n",
      " 'GBR' 'CZE' nan]\n"
     ]
    }
   ],
   "source": [
    "for col in df_train_raw.columns:\n",
    "    if not df_train_raw[col].dtype.kind in 'iuf' and not col=='NAME' and not col=='FIRST_NAME':\n",
    "        print('Distinct values in', col, ':', df_train_raw[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Employee ID                      0\n",
       "NAME                             0\n",
       "FIRST_NAME                       0\n",
       "GENDER                           0\n",
       "MANAGER                         31\n",
       "EMPLOYEE_TYPE                    0\n",
       "DEPARTMENT                       0\n",
       "DPT_CHANGE_FLAG                  0\n",
       "JOB                              0\n",
       "STATUS                           0\n",
       "COMPANY                          0\n",
       "SITE                             0\n",
       "PERMANENT                        0\n",
       "EMPLOYEE_CLASS                 120\n",
       "FULL_TIME                        0\n",
       "EMPLOYEE_LEVEL                   1\n",
       "HANDICAP                      4729\n",
       "CITIZENSHIP                      2\n",
       "AGE                              0\n",
       "CONTRACT_TENURE                  0\n",
       "EMPLOYEE_TENURE                  0\n",
       "SUM_BONUS_UNEXPECTED_3Mago     840\n",
       "SUM_BONUS_WELCOME_3Mago        840\n",
       "SUM_BONUS_CHALLENGE_3Mago      840\n",
       "SUM_BONUS_MISC_3Mago           840\n",
       "SUM_BONUS_EXC_3Mago            840\n",
       "SUM_BONUS_LANGUAGE_3Mago       842\n",
       "SUM_BONUS_SHARING_3Mago        840\n",
       "SUM_BONUS_OBJECTIVE_3Mago      840\n",
       "SUM_BONUS_YIELD_3Mago          840\n",
       "SUM_BONUS_TECHNICAL_3Mago      840\n",
       "SUM_BONUS_TOTAL_3Mago          840\n",
       "SUM_BONUS_UNEXPECTED           158\n",
       "SUM_BONUS_WELCOME              158\n",
       "SUM_BONUS_CHALLENGE            158\n",
       "SUM_BONUS_MISC                 158\n",
       "SUM_BONUS_EXC                  158\n",
       "SUM_BONUS_LANGUAGE             158\n",
       "SUM_BONUS_SHARING              158\n",
       "SUM_BONUS_OBJECTIVE            158\n",
       "SUM_BONUS_YIELD                158\n",
       "SUM_BONUS_TECHNICAL            158\n",
       "SUM_BONUS_TOTAL                158\n",
       "EVOLUTION_BONUS_CHALLENGE     4000\n",
       "EVOLUTION_BONUS_EXC           4735\n",
       "EVOLUTION_BONUS_LANGUAGE      4581\n",
       "EVOLUTION_BONUS_MISC          4741\n",
       "EVOLUTION_BONUS_OBJECTIVE     2850\n",
       "EVOLUTION_BONUS_SHARING       4721\n",
       "EVOLUTION_BONUS_TECHNICAL     4740\n",
       "EVOLUTION_BONUS_TOTAL         1330\n",
       "EVOLUTION_BONUS_UNEXPECTED    4719\n",
       "EVOLUTION_BONUS_WELCOME       4730\n",
       "EVOLUTION_BONUS_YIELD         2806\n",
       "Target_Churn                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaN\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming convention: replace NaN with -1 in columns that are not meant to be numeric\n",
    "df_train['MANAGER'].fillna(-1, inplace=True)\n",
    "df_train['DEPARTMENT'].fillna(-1, inplace=True)\n",
    "df_train['COMPANY'].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type conversions\n",
    "df_train['Employee ID'] = df_train['Employee ID'].astype('str')\n",
    "df_train['MANAGER'] = df_train['MANAGER'].astype('int').astype('str')\n",
    "df_train['DEPARTMENT'] = df_train['DEPARTMENT'].astype('int').astype('str')\n",
    "df_train['COMPANY'] = df_train['COMPANY'].astype('int').astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>\n",
    "<b>Data exploration</b><br>\n",
    "Check occurrence of values for a feature to check how evenly distributed they are.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value occurrences in EMPLOYEE_CLASS :\n",
      "EMPLOYEE_CLASS\n",
      "INT    4618\n",
      "IMP       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Value occurrences in EMPLOYEE_LEVEL :\n",
      "EMPLOYEE_LEVEL\n",
      "C    2034\n",
      "A    1642\n",
      "B    1059\n",
      "T       5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Value occurrences in CITIZENSHIP :\n",
      "CITIZENSHIP\n",
      "FRA    4620\n",
      "BEL      59\n",
      "DEU      22\n",
      "IRL      14\n",
      "SWE       5\n",
      "LUX       5\n",
      "ESP       5\n",
      "PRT       2\n",
      "NDL       2\n",
      "SRB       1\n",
      "TUN       1\n",
      "MAR       1\n",
      "GBR       1\n",
      "CZE       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Value occurrences in MANAGER :\n",
      "MANAGER\n",
      "7376.0     71\n",
      "32204.0    44\n",
      "2139.0     40\n",
      "27370.0    31\n",
      "31874.0    31\n",
      "           ..\n",
      "2581.0      1\n",
      "28997.0     1\n",
      "34310.0     1\n",
      "20857.0     1\n",
      "1537.0      1\n",
      "Name: count, Length: 384, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check occurrence of each value in columns with NaN\n",
    "nan_cols = ['EMPLOYEE_CLASS', 'EMPLOYEE_LEVEL', 'CITIZENSHIP', 'MANAGER']\n",
    "\n",
    "for col in nan_cols:\n",
    "    print(\"Value occurrences in\", col, \":\")\n",
    "    print(df_train_raw[col].value_counts())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dataframe with only master data\n",
    "df_masterdata = df_train_raw.copy()\n",
    "\n",
    "# Remove master data columns not relevant when searching for duplicates\n",
    "df_masterdata.drop(['Employee ID', 'NAME', 'FIRST_NAME', 'MANAGER', 'DEPARTMENT', 'Target_Churn'], axis='columns', inplace=True)\n",
    "\n",
    "# Remove 'bonus' columns, also not relevant when searching for duplicates\n",
    "for col in df_masterdata.columns:\n",
    "    if 'BONUS' in col:\n",
    "        df_masterdata.drop(col, axis='columns', inplace=True)\n",
    "\n",
    "for col in nan_cols:\n",
    "    df_corr = df_masterdata.copy()\n",
    "\n",
    "    # Drop lines with NaN\n",
    "    df_corr.dropna(subset=col, inplace=True)\n",
    "    \n",
    "    # Dummies\n",
    "    df_corr = pd.get_dummies(df_corr, columns=df_corr.columns,  drop_first=True)\n",
    "\n",
    "    # Correlation matrix\n",
    "    df_corr = df_corr.corr()\n",
    "\n",
    "    for corr in df_corr.columns.values.tolist():\n",
    "        if col in corr:\n",
    "            idx = df_corr.columns.values.tolist().index(corr)\n",
    "            print(col, corr, \"index=\", idx)\n",
    "            corr_col = df_corr.iloc[idx, :]\n",
    "#            print(corr_col)\n",
    "            print(df_corr.nlargest(5, corr).iloc[:, idx])\n",
    "            print(df_corr.nsmallest(5, corr).iloc[:, idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmap indicates that age, contract tenure and employee tenure have low correlations\n",
    "for col in df_masterdata.columns:\n",
    "    if 'AGE' in col or 'TENURE' in col:\n",
    "        df_masterdata.drop(col, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup = df_train.copy()\n",
    "#df_train = df_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts before clean-up:\n",
      "MANAGER\n",
      "7376     71\n",
      "32204    44\n",
      "2139     40\n",
      "31874    31\n",
      "-1       31\n",
      "         ..\n",
      "2581      1\n",
      "28997     1\n",
      "34310     1\n",
      "20857     1\n",
      "1537      1\n",
      "Name: count, Length: 385, dtype: int64 \n",
      "\n",
      "       col  nan  fill  mult  nodup\n",
      "0  MANAGER    0     0     0      0 \n",
      "\n",
      "Value counts after clean-up:\n",
      "MANAGER\n",
      "7376     71\n",
      "32204    44\n",
      "2139     40\n",
      "31874    31\n",
      "-1       31\n",
      "         ..\n",
      "2581      1\n",
      "28997     1\n",
      "34310     1\n",
      "20857     1\n",
      "1537      1\n",
      "Name: count, Length: 385, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Value counts before clean-up:\")\n",
    "print(df_train['MANAGER'].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "dup_colset = ['DEPARTMENT', 'COMPANY', 'SITE']\n",
    "nan_cols = ['MANAGER']\n",
    "\n",
    "df_stats = fill_NaN_via_dupl(df_train, nan_cols, dup_colset)\n",
    "\n",
    "print(df_stats, \"\\n\")\n",
    "\n",
    "print(\"Value counts after clean-up:\")\n",
    "print(df_train['MANAGER'].value_counts(dropna=False), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts before clean-up:\n",
      "EMPLOYEE_CLASS\n",
      "INT    4618\n",
      "NaN     120\n",
      "IMP       3\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "EMPLOYEE_LEVEL\n",
      "C      2034\n",
      "A      1642\n",
      "B      1059\n",
      "T         5\n",
      "NaN       1\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "              col  nan  fill  mult  nodup\n",
      "0  EMPLOYEE_CLASS  120   115     1      4\n",
      "1  EMPLOYEE_LEVEL    1     0     1      0 \n",
      "\n",
      "Value counts after clean-up:\n",
      "EMPLOYEE_CLASS\n",
      "INT    4733\n",
      "NaN       5\n",
      "IMP       3\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "EMPLOYEE_LEVEL\n",
      "C      2034\n",
      "A      1642\n",
      "B      1059\n",
      "T         5\n",
      "NaN       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Value counts before clean-up:\")\n",
    "print(df_train['EMPLOYEE_CLASS'].value_counts(dropna=False), \"\\n\")\n",
    "print(df_train['EMPLOYEE_LEVEL'].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "dup_colset = ['EMPLOYEE_TYPE', 'DEPARTMENT', 'JOB', 'COMPANY', 'SITE', 'PERMANENT', 'EMPLOYEE_CLASS', 'EMPLOYEE_LEVEL']\n",
    "nan_cols = ['EMPLOYEE_CLASS', 'EMPLOYEE_LEVEL']\n",
    "\n",
    "df_stats = fill_NaN_via_dupl(df_train, nan_cols, dup_colset)\n",
    "\n",
    "print(df_stats, \"\\n\")\n",
    "\n",
    "print(\"Value counts after clean-up:\")\n",
    "print(df_train['EMPLOYEE_CLASS'].value_counts(dropna=False), \"\\n\")\n",
    "print(df_train['EMPLOYEE_LEVEL'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of columns we want to use to derive a fill value for the NaN\n",
    "dup_colset = ['DEPARTMENT', 'COMPANY', 'SITE', 'MANAGER']\n",
    "# List of columns in which we want to replace the NaN\n",
    "nan_cols = ['MANAGER']\n",
    "\n",
    "df_subset = df_train[dup_colset].copy()\n",
    "i = len(df_subset.columns)\n",
    "while i < 10:\n",
    "    i += 1\n",
    "    col_name = 'EMPTY' + str(i)\n",
    "    print(col_name)\n",
    "    df_subset[col_name] = \"0\"\n",
    "    \n",
    "for col in nan_cols:\n",
    "    count_fill = 0\n",
    "    count_left = 0\n",
    "    count_false = 0\n",
    "    count_all = 0\n",
    "    print(\"Value counts before clean-up:\")\n",
    "    print(df_train[col].value_counts(dropna=False))\n",
    "    dup_cols = dup_colset.copy()\n",
    "    dup_cols.remove(col)\n",
    "    \n",
    "    # Build a Series which indicates for each line in DF_TRAIN whether it has a duplicate row \n",
    "    # considering all columns except COL\n",
    "    ser_dup = df_subset.duplicated(subset=dup_cols, keep=False)\n",
    "\n",
    "    # Build a list with the index of all rows in DF_TRAIN that have NaN for COL\n",
    "    lst_na = df_subset.loc[pd.isna(df_train[col]), :].index\n",
    "\n",
    "    for i in lst_na:\n",
    "        count_all += 1\n",
    "    \n",
    "        # For each row where COL == NaN, check if it has a duplicate\n",
    "        if ser_dup.iloc[i] == True:\n",
    "            \n",
    "            # Row has duplicate.\n",
    "            # SEARCH_LINE is the row for which we aim to remove the NaN\n",
    "            search_line = df_subset.iloc[i, :]\n",
    "\n",
    "            # Build dataframe with all duplicate lines for SEARCH_LINE ignoring COL\n",
    "            df_dup = df_subset.loc[(df_subset[dup_cols[0]] == search_line[dup_cols[0]]) &   \n",
    "                                   (df_subset[dup_cols[1]] == search_line[dup_cols[1]]) &\n",
    "                                   (df_subset[dup_cols[2]] == search_line[dup_cols[2]]) &\n",
    "                                   (df_subset[dup_cols[3]] == search_line[dup_cols[3]]) &\n",
    "                                   (df_subset[dup_cols[4]] == search_line[dup_cols[4]]) &\n",
    "                                   (df_subset[dup_cols[5]] == search_line[dup_cols[5]]) &\n",
    "                                   (df_subset[dup_cols[6]] == search_line[dup_cols[6]])\n",
    "                                    ]\n",
    "\n",
    "            # Check if all lines in DF_DUP have same value in COL, if so we assume we can replace NaN with this value\n",
    "            if len(df_dup[col].value_counts()) == 1:\n",
    "                # All duplicate lines have the same value for the NaN field so we replace NaN with this value\n",
    "#                 df_train.iloc[i, df_subset.columns.tolist().index(col)] = \\\n",
    "#                                   df_dup[df_dup[col].notna()].iloc[0, df_train.columns.tolist().index(col)]\n",
    "                count_fill += 1\n",
    "            else:\n",
    "                # Multiple values for the NaN field, result is inconclusive, NaN is left\n",
    "                count_left += 1     \n",
    "\n",
    "        else:\n",
    "            # No duplicate row for SEARCH_LINE, NaN is left\n",
    "            count_false += 1\n",
    "\n",
    "    print(\"\\nValue counts after clean-up:\")\n",
    "    print(df_train[col].value_counts(dropna=False))\n",
    "    print(\"Total NaN:\", count_all, \", NaN replaced:\", count_fill, \\\n",
    "          \", Inconclusive (NaN left):\", count_left, \", No duplicate row (NaN left):\", count_false, \".\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Employee ID                      0\n",
       "NAME                             0\n",
       "FIRST_NAME                       0\n",
       "GENDER                           0\n",
       "MANAGER                          0\n",
       "EMPLOYEE_TYPE                    0\n",
       "DEPARTMENT                       0\n",
       "DPT_CHANGE_FLAG                  0\n",
       "JOB                              0\n",
       "STATUS                           0\n",
       "COMPANY                          0\n",
       "SITE                             0\n",
       "PERMANENT                        0\n",
       "EMPLOYEE_CLASS                   5\n",
       "FULL_TIME                        0\n",
       "EMPLOYEE_LEVEL                   1\n",
       "HANDICAP                      4729\n",
       "CITIZENSHIP                      2\n",
       "AGE                              0\n",
       "CONTRACT_TENURE                  0\n",
       "EMPLOYEE_TENURE                  0\n",
       "SUM_BONUS_UNEXPECTED_3Mago     840\n",
       "SUM_BONUS_WELCOME_3Mago        840\n",
       "SUM_BONUS_CHALLENGE_3Mago      840\n",
       "SUM_BONUS_MISC_3Mago           840\n",
       "SUM_BONUS_EXC_3Mago            840\n",
       "SUM_BONUS_LANGUAGE_3Mago       842\n",
       "SUM_BONUS_SHARING_3Mago        840\n",
       "SUM_BONUS_OBJECTIVE_3Mago      840\n",
       "SUM_BONUS_YIELD_3Mago          840\n",
       "SUM_BONUS_TECHNICAL_3Mago      840\n",
       "SUM_BONUS_TOTAL_3Mago          840\n",
       "SUM_BONUS_UNEXPECTED           158\n",
       "SUM_BONUS_WELCOME              158\n",
       "SUM_BONUS_CHALLENGE            158\n",
       "SUM_BONUS_MISC                 158\n",
       "SUM_BONUS_EXC                  158\n",
       "SUM_BONUS_LANGUAGE             158\n",
       "SUM_BONUS_SHARING              158\n",
       "SUM_BONUS_OBJECTIVE            158\n",
       "SUM_BONUS_YIELD                158\n",
       "SUM_BONUS_TECHNICAL            158\n",
       "SUM_BONUS_TOTAL                158\n",
       "EVOLUTION_BONUS_CHALLENGE     4000\n",
       "EVOLUTION_BONUS_EXC           4735\n",
       "EVOLUTION_BONUS_LANGUAGE      4581\n",
       "EVOLUTION_BONUS_MISC          4741\n",
       "EVOLUTION_BONUS_OBJECTIVE     2850\n",
       "EVOLUTION_BONUS_SHARING       4721\n",
       "EVOLUTION_BONUS_TECHNICAL     4740\n",
       "EVOLUTION_BONUS_TOTAL         1330\n",
       "EVOLUTION_BONUS_UNEXPECTED    4719\n",
       "EVOLUTION_BONUS_WELCOME       4730\n",
       "EVOLUTION_BONUS_YIELD         2806\n",
       "Target_Churn                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaN\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17 % of lines dropped.\n"
     ]
    }
   ],
   "source": [
    "# Handle NaN in master data\n",
    "start_point = len(df_train)\n",
    "df_train['HANDICAP'].fillna('NONE', inplace=True)\n",
    "df_train.dropna(subset=['EMPLOYEE_CLASS', 'EMPLOYEE_LEVEL', 'CITIZENSHIP'], inplace=True)\n",
    "print(round((1 - (len(df_train) / start_point)) * 100, 2), \"% of lines dropped.\")\n",
    "#df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage there are no more NaN among the master data.  Now the bonuses..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN for a bonus corresponds with no bonus, i.e. = 0.\n",
    "for col in df_train.columns:\n",
    "    if 'BONUS' in col:\n",
    "        df_train[col].fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find redundant features\n",
    "DCF = DropConstantFeatures(tol=0.99)   # Feature is considered constant if 99% of values are identical\n",
    "DCF.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCF.features_to_drop_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VANAF HIER VERDER WERKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap indicates that age, contract tenure and employee tenure have low correlations\n",
    "for col in df_masterdata.columns:\n",
    "    if 'AGE' in col or 'TENURE' in col:\n",
    "        df_masterdata.drop(col, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    # Correlation heatmap\n",
    "#    fig, axes = plt.subplots(figsize=(20,3))\n",
    "#    sns.heatmap(df_corr.corr(), annot=True)\n",
    "    \n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check which columns are relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amt_feat = []\n",
    "cat_feat = []\n",
    "num_feat = []\n",
    "\n",
    "for col in df_train.columns:\n",
    "    if 'BONUS' in col:\n",
    "        amt_feat.append(col)\n",
    "    if not df_train[col].dtype.kind in 'iuf':\n",
    "        df_train[col] = df_train[col].astype('category')\n",
    "        cat_feat.append(col)\n",
    "    if col=='AGE' or col=='CONTRACT_TENURE' or col=='EMPLOYEE_TENURE':\n",
    "        num_feat.append(col)\n",
    "\n",
    "#col_trans = ColumnTransformer(transformers=[\n",
    "#    ('categories', OneHotEncoder(dtype='int', handle_unknown='error'), cat_feat),\n",
    "##    ('scaler', StandardScaler(), amt_feat)\n",
    "#    ])\n",
    "\n",
    "#col_trans.fit_transform(df_train)\n",
    "df_train.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['Target_Churn', 'Employee ID', 'NAME', 'FIRST_NAME'], axis='columns')\n",
    "y = df_train['Target_Churn']\n",
    "\n",
    "cat_feat_2 = cat_feat.copy()\n",
    "cat_feat_2.remove('Employee ID')\n",
    "cat_feat_2.remove('NAME')\n",
    "cat_feat_2.remove('FIRST_NAME')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('num', StandardScaler(), num_feat),\n",
    "        ('cat', OneHotEncoder(drop='if_binary', sparse=False, dtype=np.intc), cat_feat_2),\n",
    "    ], remainder='passthrough'   # By default, all un-transformed columns are dropped.\n",
    ")\n",
    "\n",
    "scaled_X = col_trans.fit_transform(X)\n",
    "\n",
    "#scaled_X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col_trans.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()   # PCA on all features\n",
    "principal_components = pca.fit_transform(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = []\n",
    "\n",
    "for n in range(1,11):\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(scaled_X)\n",
    "    \n",
    "    explained_variance.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.plot(range(1,11), explained_variance)\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Variance Explained\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca.n_components_\n",
    "#pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relatie tussen PCA en features:\n",
    "pca_comp = pd.DataFrame(pca.components_, index=['PC1','PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])\n",
    "pca_comp.columns = col_trans.get_feature_names_out()   # Replace generated column headers with actual feature names\n",
    "pca_comp.head()\n",
    "\n",
    "#plt.figure(figsize=(20,3),dpi=100)\n",
    "#sns.heatmap(pca_comp,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance mee in rekening brengen bij correlatie van de features\n",
    "display(pca.explained_variance_ratio_)\n",
    "\n",
    "pca_w = pca_comp.mul((pca.explained_variance_ratio_), axis='index').copy()\n",
    "\n",
    "## Normaliseren\n",
    "#pca_w = pca_w / pca_w.sum().abs().sum()   # Totaal van alle PCA correlaties op 1 brengen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_wabs = pca_w.abs().copy()\n",
    "\n",
    "# Calculate total correlation per feature rather than by OneHotEncoder split\n",
    "lst_cum = ['MANAGER', 'DEPARTMENT', 'DPT_CHANGE_FLAG', 'COMPANY', 'SITE', 'FULL_TIME', 'EMPLOYEE_LEVEL', 'HANDICAP', 'CITIZENSHIP']\n",
    "for col in lst_cum:\n",
    "    prefix = 'cat__' + col\n",
    "\n",
    "    # Add column with total correlation\n",
    "    pca_wabs[col] = pca_wabs.filter(regex=prefix, axis='columns').sum(axis='columns')\n",
    "\n",
    "    # Remove columns with correlations for individual OneHotEncoder splits\n",
    "    fltr = filter(lambda x: x.startswith(prefix), pca_wabs.columns)\n",
    "    pca_wabs.drop(list(fltr), axis='columns', inplace=True)\n",
    "\n",
    "# Remove prefixes from column names\n",
    "dict_colnames = {}\n",
    "for i, col in enumerate(pca_wabs.columns):\n",
    "    if col.find('__') > 0:\n",
    "        dict_colnames[col] = col[ (col.find('__')+2) : ]\n",
    "\n",
    "pca_wabs.rename(columns=dict_colnames, inplace=True)\n",
    "\n",
    "display(pca_wabs.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF opbouwen met gecumuleerde feature importances over de 10 PC's\n",
    "pca_feat_imp = pd.DataFrame(data={'Importance' : pca_w.sum()}, index=col_trans.get_feature_names_out())\n",
    "\n",
    "# Features aggregeren (undo OneHotEncoding)\n",
    "pca_feat_imp = agg_feat_imp(pca_feat_imp)\n",
    "\n",
    "# Features plotten volgens afnemend belang\n",
    "plot_feat_imp(pca_feat_imp, export=True, file_name='Output/FeatureImportance_PrincipalComponents.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features sorteren volgens afnemend gecumuleerd belang over de 10 PC\n",
    "pca_feat_imp = pd.DataFrame(data={'Importance' : pca_wabs.sum()}, index=pca_wabs.columns)\n",
    "pca_feat_imp.sort_values('Importance', ascending=False, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12,6),dpi=100)\n",
    "sns.barplot(pca_feat_imp, x=pca_feat_imp.index, y=pca_feat_imp['Importance'])\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_rfc = False\n",
    "\n",
    "if switch_rfc:\n",
    "    n_estimators=[100, 128, 15, 200, 250, 300, 350]\n",
    "    max_features= [2, 4, 6, 8, 10]\n",
    "    bootstrap = [True]\n",
    "    oob_score = [True]   # oob_score heeft geen impact op performantie, geeft enkel idee van accuraatheid van model\n",
    "    # Best params na 1e run: {'bootstrap': True, 'max_features': 8, 'n_estimators': 128, 'oob_score': True}\n",
    "\n",
    "    param_grid = {'n_estimators':n_estimators,\n",
    "                'max_features':max_features,\n",
    "                'bootstrap':bootstrap,\n",
    "                'oob_score':oob_score}  # oob_score is enkel zinvol indien bootstrap=True anders zal de fit failen\n",
    "\n",
    "    rfc = RandomForestClassifier()\n",
    "    grid = GridSearchCV(rfc,param_grid)\n",
    "    grid.fit(scaled_X, y)\n",
    "\n",
    "    display(grid.best_params_)   # parameters beste estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aanmaken beste model met oob_score:\n",
    "rfc = RandomForestClassifier(max_features=8, n_estimators=300, oob_score=True)\n",
    "rfc.fit(scaled_X, y)\n",
    "rfc.oob_score_   # oob_score is tegenovergestelde van OOB error !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF opbouwen met feature importances\n",
    "rfc_feat_imp = pd.DataFrame(data={'Importance' : rfc.feature_importances_}, index=col_trans.get_feature_names_out())\n",
    "\n",
    "# Features aggregeren (undo OneHotEncoding)\n",
    "rfc_feat_imp = agg_feat_imp(rfc_feat_imp)\n",
    "\n",
    "# Features plotten volgens afnemend belang\n",
    "plot_feat_imp(rfc_feat_imp, export=True, file_name='Output/FeatureImportance_RandomForest.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_gb = False\n",
    "if switch_gb:\n",
    "    n_est = [80, 100, 150, 200, 300]\n",
    "    max_f = [6, 8, 10]\n",
    "    max_d = [4, 5, 6]\n",
    "    l_rate = [0.02, 0.05, 0.1]\n",
    "    # Best params na 1e run: {'learning_rate': 0.05, 'max_depth': 5, 'max_features': 8, 'n_estimators': 100}\n",
    "    # Best params na 2e run: {'learning_rate': 0.05, 'max_depth': 4, 'max_features': 6, 'n_estimators': 200}\n",
    "\n",
    "    param_grid = {'n_estimators': n_est,\n",
    "                'max_features': max_f,\n",
    "                'max_depth': max_d,\n",
    "                'learning_rate': l_rate}\n",
    "\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    grid = GridSearchCV(gbc, param_grid)\n",
    "    grid.fit(scaled_X, y)\n",
    "\n",
    "    display(grid.best_params_)   # parameters beste estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(learning_rate= 0.05, max_depth= 5, max_features= 8, n_estimators= 200)\n",
    "gbc.fit(scaled_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF opbouwen met feature importances\n",
    "gbc_feat_imp = pd.DataFrame(data=gbc.feature_importances_, index=col_trans.get_feature_names_out(), columns=['Importance'])\n",
    "\n",
    "# Features aggregeren (undo OneHotEncoding)\n",
    "gbc_feat_imp = agg_feat_imp(gbc_feat_imp)\n",
    "\n",
    "# Features plotten volgens afnemend belang\n",
    "plot_feat_imp(gbc_feat_imp, export=True, file_name='Output/FeatureImportance_GradientBoosting.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correlation between bonuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate dataframes with bonuses\n",
    "df_3m_bonus = pd.DataFrame()\n",
    "df_sumbonus = pd.DataFrame()\n",
    "df_evobonus = pd.DataFrame()\n",
    "\n",
    "for col in amt_feat:\n",
    "    if 'SUM_BONUS' in col:\n",
    "        if '3Mago' in col:\n",
    "            df_3m_bonus[col] = df_train[col]\n",
    "        else:\n",
    "            df_sumbonus[col] = df_train[col]\n",
    "    if 'EVOLUTION' in col:\n",
    "        df_evobonus[col] = df_train[col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for bonus columns with only values = 0 & remove them\n",
    "switch_del_col_0 = False\n",
    "\n",
    "if switch_del_col_0:\n",
    "    print(len(df_3m_bonus.columns))\n",
    "    zero_cols = df_3m_bonus.columns[(df_3m_bonus == 0).all()]\n",
    "    df_3m_bonus.drop(labels=zero_cols, axis=1, inplace=True)\n",
    "    print(len(df_3m_bonus.columns))\n",
    "\n",
    "    print(len(df_sumbonus.columns))\n",
    "    zero_cols = df_sumbonus.columns[(df_sumbonus == 0).all()]\n",
    "    df_sumbonus.drop(labels=zero_cols, axis=1, inplace=True)\n",
    "    print(len(df_sumbonus.columns))\n",
    "\n",
    "    print(len(df_evobonus.columns))\n",
    "    zero_cols = df_evobonus.columns[(df_evobonus == 0).all()]\n",
    "    df_evobonus.drop(labels=zero_cols, axis=1, inplace=True)\n",
    "    print(len(df_evobonus.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus correlation matrices\n",
    "df_sumcorr = df_sumbonus.corr()\n",
    "df_3m_corr = df_3m_bonus.corr()\n",
    "df_evocorr = df_evobonus.corr()\n",
    "\n",
    "# Trim column headers\n",
    "df_sumcorr.rename(columns=lambda x: x.removeprefix('SUM_BONUS_'), inplace=True)\n",
    "df_3m_corr.rename(columns=lambda x: x.removeprefix('SUM_BONUS_'), inplace=True)\n",
    "df_3m_corr.rename(columns=lambda x: x.removesuffix('_3Mago'), inplace=True)\n",
    "df_evocorr.rename(columns=lambda x: x.removeprefix('EVOLUTION_BONUS_'), inplace=True)\n",
    "\n",
    "# Trim index\n",
    "df_sumcorr.rename(index=lambda x: x.removeprefix('SUM_BONUS_'), inplace=True)\n",
    "df_3m_corr.rename(index=lambda x: x.removeprefix('SUM_BONUS_'), inplace=True)\n",
    "df_3m_corr.rename(index=lambda x: x.removesuffix('_3Mago'), inplace=True)\n",
    "df_evocorr.rename(index=lambda x: x.removeprefix('EVOLUTION_BONUS_'), inplace=True)\n",
    "\n",
    "# Sort DF rows identically\n",
    "df_3m_corr = df_3m_corr.reindex(labels=df_sumcorr.columns.tolist())\n",
    "df_evocorr = df_evocorr.reindex(labels=df_sumcorr.columns.tolist())\n",
    "\n",
    "# Sort DF columns identically\n",
    "df_3m_corr = df_3m_corr[df_sumcorr.columns.tolist()]\n",
    "df_evocorr = df_evocorr[df_sumcorr.columns.tolist()]\n",
    "\n",
    "# Print correlation matrices in a single row\n",
    "from IPython.display import display_html \n",
    "df1_styler = df_sumcorr.style.set_table_attributes(\"style='display:inline', margin-right:20px;'\").set_caption('Correlations Sum Bonus')\n",
    "df2_styler = df_3m_corr.style.set_table_attributes(\"style='display:inline', margin-right:20px;'\").set_caption('Correlations 3m Bonus')\n",
    "df3_styler = df_evocorr.style.set_table_attributes(\"style='display:inline'\").set_caption('Correlations Evo Bonus')\n",
    "display_html(df1_styler._repr_html_()+df2_styler._repr_html_()+df3_styler._repr_html_(), raw=True)\n",
    "\n",
    "# Print correlation heatmaps in single row\n",
    "fig, axes = plt.subplots(figsize=(20,3), dpi=100, nrows=1, ncols=3)\n",
    "sns.heatmap(df_sumcorr, ax=axes[0])   # annot=True, \n",
    "sns.heatmap(df_3m_corr, ax=axes[1])   # annot=True, \n",
    "sns.heatmap(df_evocorr, ax=axes[2])   # annot=True, \n",
    "axes[0].set_title('Correlations Sum Bonus')\n",
    "axes[1].set_title('Correlations Sum Bonus 3m')\n",
    "axes[2].set_title('Correlations Evo Bonus')\n",
    "plt.subplots_adjust(wspace=0.8)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim column headers\n",
    "df_sumbonus.rename(columns=lambda x: x.removeprefix('SUM_BONUS_'), inplace=True)\n",
    "df_3m_bonus.rename(columns=lambda x: x.removeprefix('SUM_BONUS_'), inplace=True)\n",
    "df_3m_bonus.rename(columns=lambda x: x.removesuffix('_3Mago'), inplace=True)\n",
    "#df_evocorr.rename(columns=lambda x: x.removeprefix('EVOLUTION_BONUS_'), inplace=True)\n",
    "\n",
    "df_sumbonus.corrwith(df_3m_bonus, axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_condensed = df_train.copy()\n",
    "# Ignore names\n",
    "df_train_condensed.drop(['NAME', 'FIRST_NAME'], axis=1, inplace=True)\n",
    "# Drop columns with only a single value\n",
    "df_train_condensed.drop(['EMPLOYEE_TYPE', 'JOB', 'STATUS'], axis=1, inplace=True)\n",
    "# Ignore bonus details\n",
    "for col in df_train_condensed.columns:\n",
    "    if ( 'SUM_BONUS' in col or 'EVOLUTION_BONUS' in col ) and not 'TOTAL' in col:\n",
    "        del df_train_condensed[col]\n",
    "df_train_condensed.head()\n",
    "#sns.pairplot(df_train_condensed,diag_kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train | Test Split en Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop('Target_Churn',axis=1)\n",
    "X = X.drop(['Employee ID', 'NAME', 'FIRST_NAME'], axis='columns')\n",
    "y = df_train['Target_Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maken van het Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_model.fit(scaled_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Age is negatief: odds om tot klasse 1 voor test_result te behoren nemen af met de leeftijd, maar nemen toe met physical_score omdat de coëfficiënt daar positief is.  Op basis van de ratio kunnen we stellen dat physical_score een sterkere voorspeller is dan age.  Dit zagen we ook in de boxplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performantie voor Classificatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = log_model.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# herinner de accuracy paradox: we willen niet alleen op deze metric vertrouwen!\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Deze score zegt ons dat, gegeven iemands leeftijd en fysieke test resultaat, in 93% van de gevallen we correct kunnen voorspellen of iemand voor de test zal slagen."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Confusion matrix: True Positives, False Positives, False Negatives, True Negatives:\n",
    "#\n",
    "#     PREDICTED\n",
    "#       | 0   |  1 \n",
    "#       |-----|----   \n",
    "#  T  0 | TN  |  FP \n",
    "#  R    |     |\n",
    "#  U    |-----|----  \n",
    "#  E  1 | FN  |  TP\n",
    "#       |-----|----\n",
    "\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Idem: confusion matrix, maar mooi geplot - Ruwe waarden\n",
    "cm = confusion_matrix(y_test, y_pred, labels=log_model.classes_)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=log_model.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix, genormaliseerd met normalize = true: \n",
    "# Dit is normalisatie over rijen, dus over de echte labels.  Normalisatie betekent dat de waarden in de rij\n",
    "# zullen worden genormaliseerd om samen een som van 1 te bekomen (100%)\n",
    "\n",
    "# Elke rij in de confusion matrix zal de proportie predicties voor de respectieve echte klasse weergeven.\n",
    "# Dit geeft, voor elke echte klasse (0 / 1), welke proportie van de samples voor elke klasse werden voorspeld\n",
    "# Concreet: de eerste rij geeft weer dat, van alle echte klasse 0 samples, 89% correct werd voorspeld, en 11% \n",
    "# foutief aan klasse 1 werden toegekend.\n",
    "# De tweede rij geeft aan dat 5% van de echte klasse 1 samples foutief als klasse 0 werden voorspeld, en 95% juist.\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=log_model.classes_, normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=log_model.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Confusion matrix, genormaliseerd met normalize = pred: \n",
    "# Dit is een normalisatie op kolom, dus normalisatie zodat de som van de predicties per klasse 1 geven (dus over kolommen) \n",
    "# Concreet voor elke kolom: de waarden representeren de proportie van die voorspelde klasse die tot elke werkelijke klasse behoren.\n",
    "# Maw: het leert ons, voor een gegeven voorspelde klasse, welk percentage van de voorspellingen correct waren, en welk % misclassificaties\n",
    "# van andere klasses waren.\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=log_model.classes_, normalize='pred')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=log_model.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Confusion matrix, genormaliseerd met normalize = all: \n",
    "# Dit is een normalisatie op totaal van de samples.  Elke waarde in de cm zal worden gedeeld door het totaal aantal\n",
    "# samples om zo de proportie van het totaal weer te geven, en dus de proportie True Negatives etc...\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=log_model.classes_, normalize='all')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=log_model.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Precision, Recall en F1-score:\n",
    "# vergeet het print statement niet hier...\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We krijgen precision, recall en f1-score per klasse!\n",
    "recall class 1: TP / TP + FN = 293 / (293 + 14)\n",
    "...\n",
    "De support kolom geeft weer hoeveel instanties van elke klasse aanwezig waren in de test set om deze data te ondersteunen.  Hier zien we dat de set redelijk gebalanceerd is (193 / 307)\n",
    "\n",
    "! Als je waarden voor precision en recall voor de individuale klasses redelijk overeenkomen met de algemene accuracy, dan is er geen probleem inzake ongebalanceerde data.\n",
    "\n",
    "In een classification_report van sklearn, staan de termen macro avg en weighted avg voor verschillende manieren om gemiddelde scores te berekenen over de verschillende klassen. Ze zijn vooral nuttig bij multi-class classificatieproblemen. Hier is een uitleg van elk:\n",
    "\n",
    "Macro avg (Macro Gemiddelde):\n",
    "Voor elke metriek (bijv. precisie, recall, f1-score) wordt het gemiddelde berekend zonder rekening te houden met de class-imbalance (het aantal samples in elke klasse). Het behandelt elke klasse gelijk.\n",
    "\n",
    "Weighted avg (Gewogen Gemiddelde):\n",
    "Voor elke metriek wordt het gemiddelde berekend, waarbij rekening wordt gehouden met de class-imbalance. Elke klasse wordt gewogen naar het aantal daadwerkelijke samples.\n",
    "\n",
    "vb weighted avg precision = (precisie class 1 × # samples class 1 + precisie class 2 × # samples class 2 + ...) / totaal # samples\n",
    " \n",
    "De keuze voor het gebruik van macro of gewogen gemiddelde hangt af van wat je belangrijk vindt in je specifieke situatie:\n",
    "-Als je wilt dat elke klasse gelijk wordt behandeld, ongeacht het aantal samples in elke klasse, gebruik dan het macro gemiddelde.\n",
    "-Als je wilt dat klassen met meer samples een grotere invloed hebben op het gemiddelde, gebruik dan het gewogen gemiddelde.\n",
    "\n",
    "In situaties met sterk onevenwichtige klassen kan het macro gemiddelde een duidelijker beeld geven van de prestaties van het model op minder voorkomende klassen, terwijl het gewogen gemiddelde een algemener beeld geeft van de algehele prestaties over het hele dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Als je enkel precision en recall wil: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Als je een voorspelling voor een punt wil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_model.predict_proba(scaled_X_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# werkelijke waarde\n",
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Curves and AUC\n",
    "\n",
    "**Make sure to watch the video on this!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve,PrecisionRecallDisplay,RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tradeoff tussen precision en recall visualiseren:\n",
    "fig, ax = plt.subplots(figsize=(12,8), dpi=200)\n",
    "y_prob = log_model.predict_proba(scaled_X_test)\n",
    "y_prob = y_prob[:,1] # enkel kans klasse 1\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "disp.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fig, ax = plt.subplots(figsize=(12,8), dpi=200)\n",
    "RocCurveDisplay.from_estimator(log_model, scaled_X_test, y_test, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
